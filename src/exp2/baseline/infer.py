# -*- coding: utf-8 -*-
"""
å®éªŒ2 Baseline æ¨ç†è„šæœ¬ï¼ˆä¸ä½¿ç”¨RAGï¼‰

éœ€æ±‚å¯¹é½ï¼š
1) è·¯å¾„ä¸å†™æ­»ï¼šç»Ÿä¸€ä½¿ç”¨ src.common.pathsï¼ˆå¹¶è‡ªåŠ¨é€‰æ‹© exp2ï¼‰
2) åªè¯»å– qa_testset_500.json çš„ "query" åšæ¨ç†
3) é˜¶æ®µè¾“å‡ºè¿›åº¦ + ç¤ºä¾‹å›ç­”
4) è¾“å…¥/è¾“å‡º/æ¨¡å‹é…ç½®/å‚æ•°/è·¯å¾„ç­‰å‰ç½®ï¼Œä¸­æ–‡æ³¨é‡Š
5) ç»“æœä¿å­˜åˆ° experiments/exp2/resultsï¼Œå­—æ®µï¼šquery / answer / LLM_answer
6) è¾“å‡ºæ–‡ä»¶å‘½åï¼šanswer_æ¨¡å‹å‚æ•°ï¼ˆè¿™é‡Œï¼šanswer_qwen2.5_3b.jsonï¼‰

è¿è¡Œæ–¹å¼ï¼ˆåœ¨é¡¹ç›®æ ¹ç›®å½•ï¼‰ï¼š
  python -m src.exp2.baseline.infer
  æˆ–
  python src/exp2/baseline/infer.py
"""

from __future__ import annotations

# =========================================================
# 0) ã€å¿…é¡»ã€‘å…ˆè®¾ç½®å®éªŒé€‰æ‹©ï¼ˆå› ä¸º paths.py åœ¨ import æ—¶ä¼šè¯»å–ç¯å¢ƒå˜é‡ï¼‰
# =========================================================
import os
os.environ.setdefault("LLM_EXPERIMENT", "exp2")  # ä¿è¯ data_path / results_path æŒ‡å‘ experiments/exp2/...

# ç°åœ¨å† import pathsï¼ˆéå¸¸å…³é”®ï¼‰
from src.common.paths import data_path, prompt_path, results_path, ensure_dir  # noqa: E402

# =========================================================
# 1) ã€é…ç½®å‰ç½®åŒºã€‘è¾“å…¥/è¾“å‡º/æ¨¡å‹é…ç½®/å‚æ•°/è·¯å¾„/æ—¥å¿— å…¨éƒ¨æ”¾è¿™é‡Œ
# =========================================================
import argparse
import json
import time
from typing import Dict, List

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM


# -------------------------
# è¾“å…¥æ•°æ®é…ç½®
# -------------------------
# ä½ æˆªå›¾ä¸­ data æ–‡ä»¶åå°±æ˜¯ qa_testset_500.json
DATA_FILE = "qa_testset_500.json"   # -> experiments/exp2/data/qa_testset_500.json

# -------------------------
# Prompt é…ç½®ï¼ˆåç»­ä½ æ”¾åˆ° experiments/exp2/promptsï¼‰
# -------------------------
PROMPT_FILE = "baseline_qa.txt"     # -> experiments/exp2/prompts/baseline_qa.txt

DEFAULT_PROMPT = """ä½ æ˜¯ä¸€åæ³•å¾‹å’¨è¯¢åŠ©æ‰‹ã€‚

è¦æ±‚ï¼š
1) åªè§£é‡Šä¸€èˆ¬æ€§çš„æ³•å¾‹åŸåˆ™ä¸å»ºè®®ï¼Œä¸ç¼–é€ å…·ä½“äº‹å®ã€‚
2) ä¸æä¾›è¿æ³•/è§„é¿ç›‘ç®¡/å±é™©è¡Œä¸ºçš„å…·ä½“æ“ä½œæ­¥éª¤ã€‚
4) è¡¨è¾¾æ¸…æ™°ã€ç®€æ´ï¼Œç»“æ„ï¼šåŸåˆ™è¯´æ˜ + å»ºè®®æ–¹å‘ã€‚

ç”¨æˆ·é—®é¢˜ï¼š
{query}

è¯·ç»™å‡ºå›ç­”ï¼š
"""

# -------------------------
# æ¨¡å‹é…ç½®ï¼ˆæœ¬é¢˜è¦æ±‚ï¼šç”¨ Qwen2.5 çš„ 3B æ¨¡å‹ï¼‰
# -------------------------
MODEL_CONFIG = {
    "model_tag": "qwen2.5_3b",
    "model_id": "Qwen/Qwen2.5-3B-Instruct",
    "dtype": "float16",          # äº‘ç«¯/æ˜¾å¡é€šå¸¸ç”¨ float16ï¼›CPU ä¼šè‡ªåŠ¨å›è½åˆ° float32
    "use_chat_template": True,   # Qwen instruct å»ºè®®ç”¨ chat template
}

# -------------------------
# ç”Ÿæˆå‚æ•°ï¼ˆå¯åœ¨è¿™é‡Œç»Ÿä¸€è°ƒä¼˜ï¼‰
# -------------------------
GEN_CONFIG = {
    "max_new_tokens": 256,
    "temperature": 0.2,
    "top_p": 0.8,
    "do_sample": True,
}

# -------------------------
# æ—¥å¿—/è¿›åº¦/ç¤ºä¾‹è¾“å‡º
# -------------------------
LOG_EVERY = 10
PREVIEW_EVERY = 50
PREVIEW_CHARS = 220


# =========================================================
# 2) å·¥å…·å‡½æ•°ï¼šè¯»å–prompt / è¯»å–æ•°æ® / æ„é€ è¾“å…¥ / ç”Ÿæˆå›ç­”
# =========================================================
def load_prompt_text() -> str:
    """
    ä¼˜å…ˆä» experiments/exp2/prompts/baseline_qa.txt è¯»å– promptï¼›
    å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ™ä½¿ç”¨ DEFAULT_PROMPTã€‚
    """
    p = prompt_path(PROMPT_FILE)
    if p.exists():
        txt = p.read_text(encoding="utf-8").strip()
        if "{query}" not in txt:
            raise ValueError(f"Prompt æ–‡ä»¶å¿…é¡»åŒ…å« '{{query}}' å ä½ç¬¦ï¼š{p}")
        return txt
    return DEFAULT_PROMPT


def load_testset() -> List[Dict]:
    """
    è¯»å– qa_testset_500.jsonï¼ˆJSONæ•°ç»„ï¼‰
    - æ¯æ¡è‡³å°‘åŒ…å« query
    - answer ä½œä¸ºå‚è€ƒç­”æ¡ˆä¿ç•™ï¼Œä½†ä¸ä¼šä½œä¸ºæ¨¡å‹è¾“å…¥ï¼ˆä¸¥æ ¼æ»¡è¶³è¦æ±‚2ï¼‰
    """
    p = data_path(DATA_FILE)
    if not p.exists():
        raise FileNotFoundError(f"æœªæ‰¾åˆ°æµ‹è¯•é›†æ–‡ä»¶ï¼š{p}")

    data = json.loads(p.read_text(encoding="utf-8"))
    if not isinstance(data, list):
        raise ValueError("qa_testset_500.json å¿…é¡»æ˜¯ JSON æ•°ç»„ï¼ˆlistï¼‰ã€‚")

    items = []
    for obj in data:
        if not isinstance(obj, dict):
            continue
        q = obj.get("query", "")
        if isinstance(q, str) and q.strip():
            items.append({
                "query": q.strip(),
                "answer": obj.get("answer", "")  # å‚è€ƒç­”æ¡ˆå¯ä¸ºç©º
            })

    if not items:
        raise ValueError("æµ‹è¯•é›†ä¸­æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„ 'query' å­—æ®µã€‚")
    return items


def build_model_input(tokenizer, prompt_tpl: str, query: str, use_chat_template: bool) -> str:
    """
    æ„é€ æ¨¡å‹è¾“å…¥ï¼š
    - å¦‚æœæ¨¡å‹/Tokenizer æ”¯æŒ chat_templateï¼Œåˆ™ç”¨å¯¹è¯æ¨¡æ¿ï¼ˆæ›´ç¨³ï¼‰
    - å¦åˆ™é€€åŒ–ä¸ºçº¯æ–‡æœ¬ prompt
    """
    user_text = prompt_tpl.format(query=query)

    if use_chat_template and hasattr(tokenizer, "apply_chat_template"):
        try:
            return tokenizer.apply_chat_template(
                [{"role": "user", "content": user_text}],
                tokenize=False,
                add_generation_prompt=True
            )
        except Exception:
            # è‹¥æŸäº›ç¯å¢ƒ/ç‰ˆæœ¬ä¸å…¼å®¹ï¼Œè‡ªåŠ¨å›é€€
            pass

    return user_text


@torch.inference_mode()
def generate_one(model, tokenizer, input_text: str) -> str:
    """
    å•æ¡æ¨ç†ï¼š
    åªè§£ç æ¨¡å‹â€œæ–°ç”Ÿæˆâ€çš„ tokenï¼Œ
    é¿å…æŠŠ system / user prompt ä¸€èµ·å½“æˆå›ç­”ã€‚
    """
    inputs = tokenizer(input_text, return_tensors="pt").to(model.device)

    outputs = model.generate(
        **inputs,
        max_new_tokens=GEN_CONFIG["max_new_tokens"],
        do_sample=GEN_CONFIG["do_sample"],
        temperature=GEN_CONFIG["temperature"],
        top_p=GEN_CONFIG["top_p"],
        eos_token_id=tokenizer.eos_token_id,
        pad_token_id=tokenizer.eos_token_id,
    )

    # ğŸ”‘ å…³é”®ï¼šåˆ‡æ‰è¾“å…¥éƒ¨åˆ†ï¼Œåªä¿ç•™æ–°ç”Ÿæˆå†…å®¹
    input_len = inputs["input_ids"].shape[-1]
    generated_ids = outputs[0][input_len:]

    answer = tokenizer.decode(
        generated_ids,
        skip_special_tokens=True
    )

    return answer.strip()



# =========================================================
# 3) ä¸»æµç¨‹ï¼šåŠ è½½ -> æ¨ç† -> æ—¥å¿— -> ä¿å­˜
# =========================================================
def run_baseline(model_id: str, model_tag: str, use_chat_template: bool) -> str:
    # ---------- è¯»å–æ•°æ®ä¸prompt ----------
    items = load_testset()
    prompt_tpl = load_prompt_text()

    # ---------- è¾“å‡ºè·¯å¾„ï¼ˆç¡®ä¿ç›®å½•å­˜åœ¨ï¼‰ ----------
    out_dir = ensure_dir(results_path())  # -> experiments/exp2/results
    out_file = out_dir / f"answer_{model_tag}.json"  # è¦æ±‚6ï¼šanswer_æ¨¡å‹å‚æ•°

    # ---------- è®¾å¤‡ä¸dtype ----------
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    dtype = MODEL_CONFIG.get("dtype", "float16")
    torch_dtype = torch.float16 if dtype == "float16" else torch.float32

    # ---------- æ‰“å°å…³é”®é…ç½®ï¼ˆå®éªŒæŠ¥å‘Š/å¤ç°éœ€è¦ï¼‰ ----------
    print("=" * 80)
    print("[INFO] å®éªŒï¼šExp2 Baselineï¼ˆæ— RAGï¼‰")
    print(f"[INFO] LLM_EXPERIMENT = {os.environ.get('LLM_EXPERIMENT')}")
    print(f"[INFO] æ•°æ®é›†è·¯å¾„ï¼š{data_path(DATA_FILE)} | æ ·æœ¬æ•°ï¼š{len(items)}")
    prompt_file_path = prompt_path(PROMPT_FILE)
    print(f"[INFO] Promptï¼š{prompt_file_path if prompt_file_path.exists() else 'DEFAULT_PROMPT'}")
    print(f"[INFO] æ¨¡å‹ï¼š{model_id} | tagï¼š{model_tag}")
    print(f"[INFO] è®¾å¤‡ï¼š{device} | dtypeï¼š{dtype}")
    print(f"[INFO] ç”Ÿæˆå‚æ•°ï¼š{GEN_CONFIG}")
    print(f"[INFO] è¾“å‡ºæ–‡ä»¶ï¼š{out_file}")
    print("=" * 80)

    # ---------- åŠ è½½æ¨¡å‹ ----------
    tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained(
        model_id,
        trust_remote_code=True,
        torch_dtype=torch_dtype if device.type == "cuda" else torch.float32,
        device_map="auto" if device.type == "cuda" else None,
    )
    model.eval()

    # ---------- æ¨ç†å¾ªç¯ ----------
    results: List[Dict] = []
    start_time = time.time()

    for idx, obj in enumerate(items, start=1):
        query = obj["query"]
        ref_answer = obj.get("answer", "")

        # è¦æ±‚2ï¼šåªä½¿ç”¨ queryï¼ˆä¸ä½¿ç”¨ ref_answerï¼‰
        input_text = build_model_input(tokenizer, prompt_tpl, query, use_chat_template=use_chat_template)
        llm_answer = generate_one(model, tokenizer, input_text)

        results.append({
            "query": query,
            "answer": ref_answer,
            "LLM_answer": llm_answer
        })

        # è¦æ±‚3ï¼šè¿›åº¦è¾“å‡º
        if idx % LOG_EVERY == 0 or idx == len(items):
            elapsed = time.time() - start_time
            speed = idx / elapsed if elapsed > 0 else 0.0
            print(f"[PROGRESS] {idx}/{len(items)} | {speed:.2f} æ¡/ç§’ | elapsed {elapsed:.1f}s")

        # è¦æ±‚3ï¼šç¤ºä¾‹è¾“å‡º
        if idx == 1 or idx % PREVIEW_EVERY == 0:
            pq = query[:PREVIEW_CHARS] + ("â€¦" if len(query) > PREVIEW_CHARS else "")
            pa = llm_answer[:PREVIEW_CHARS] + ("â€¦" if len(llm_answer) > PREVIEW_CHARS else "")
            print("\n[ç¤ºä¾‹è¾“å‡º]")
            print("Q:", pq)
            print("A:", pa)
            print()

    # ---------- ä¿å­˜ç»“æœ ----------
    out_file.write_text(json.dumps(results, ensure_ascii=False, indent=2), encoding="utf-8")
    print(f"[DONE] å·²ä¿å­˜ï¼š{out_file}")
    return str(out_file)


# =========================================================
# 4) CLI å…¥å£ï¼šé»˜è®¤ä½¿ç”¨ Qwen2.5-3B-Instruct
# =========================================================
def main():
    parser = argparse.ArgumentParser(description="Exp2 Baseline Inference (Qwen2.5 3B)")
    # å…è®¸ä½ å°†æ¥è¦†ç›–ï¼Œä½†é»˜è®¤å°±æ˜¯ Qwen2.5-3B-Instruct
    parser.add_argument("--model_id", type=str, default=MODEL_CONFIG["model_id"], help="HuggingFace model id")
    parser.add_argument("--model_tag", type=str, default=MODEL_CONFIG["model_tag"], help="output filename tag")
    args = parser.parse_args()

    run_baseline(
        model_id=args.model_id,
        model_tag=args.model_tag,
        use_chat_template=MODEL_CONFIG.get("use_chat_template", True),
    )


if __name__ == "__main__":
    main()
